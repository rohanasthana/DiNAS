{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nas_environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics.pairwise\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, space_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_logs_path = 'train_logs_single_run/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(precomputed_logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env._logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_set = env.get_precomputed_recepies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_resutls = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 5\n",
    "iters_per_round = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random seach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_epochs in [10, 50]:\n",
    "    all_test_losses = []\n",
    "    all_wall_times = []\n",
    "\n",
    "    N = int(iters_per_round*50/train_epochs)\n",
    "    \n",
    "    for seed in tqdm_notebook(range(rounds)):\n",
    "        np.random.seed(seed)\n",
    "        env.reset()\n",
    "        selected_inds = []\n",
    "        test_losses = []\n",
    "        wall_times = []\n",
    "        for i in range(N):\n",
    "            cur_ind = np.random.choice(np.setdiff1d(np.arange(len(search_set)), np.array(selected_inds)), \n",
    "                                       1, replace=False)[0]\n",
    "            env.simulated_train(search_set[cur_ind], train_epochs)\n",
    "            selected_inds.append(cur_ind)\n",
    "            test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "            wall_times.append(env.get_total_time())\n",
    "        all_test_losses.append(test_losses)\n",
    "        all_wall_times.append(wall_times)\n",
    "    alg_resutls[f'random_search_{train_epochs}_epochs'] = {'all_test_losses':all_test_losses, 'all_wall_times':all_wall_times}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_losses = []\n",
    "all_wall_times = []\n",
    "N = iters_per_round\n",
    "for seed in tqdm_notebook(range(rounds)):\n",
    "    env.reset()\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # HYPERBAND\n",
    "    \n",
    "    #inputs\n",
    "    R = 50 # the maximum amount of resource that can be allocated to a single configuration (number of epochs)\n",
    "    eta = 3 # an input that controls the proportion of configurations discarded in each round of SuccessiveHalving\n",
    "\n",
    "    # initialization\n",
    "    s_max = int(np.floor(np.log(R)/np.log(eta)))\n",
    "    # B = (s_max + 1)*R\n",
    "    B = N*R/3.5 # to approximately match budgets in random search\n",
    "\n",
    "    test_losses = []\n",
    "    wall_times = []\n",
    "    \n",
    "    log_cnt = 0\n",
    "    for s in range(s_max, -1, -1):\n",
    "        n = int(np.ceil(float(B)/R * float(eta)**s/(s + 1)))\n",
    "        r = R*float(eta)**(-s)\n",
    "        #print(s, n, r)\n",
    "        # Successive Halving inner loop\n",
    "        # init sample of n architectures\n",
    "        T = np.random.choice(len(search_set), n, replace=False)\n",
    "        #print(T)\n",
    "        for i in range(s + 1):\n",
    "            n_i = int(np.floor(n*float(eta)**(-i)))\n",
    "            r_i = int(np.floor(r*eta**i))\n",
    "            L = []\n",
    "            for t in T:\n",
    "                env.simulated_train(search_set[t], r_i)\n",
    "                if env.get_model_status(search_set[t]) == 'OK':\n",
    "                    L.append(env.get_model_stats(search_set[t], r_i - 1)['val_loss'])\n",
    "                else:\n",
    "                    L.append(np.inf) # if model fails accidently within r_i epichs, it is discated further\n",
    "                log_cnt += 1\n",
    "                if log_cnt % 25 == 0:\n",
    "                    test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "                    wall_times.append(env.get_total_time())\n",
    "            test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "            wall_times.append(env.get_total_time())\n",
    "\n",
    "            L = np.array(L)\n",
    "            halved_inds = np.argsort(L)[:int(np.floor(n_i/float(eta)))]\n",
    "            halved_inds = halved_inds[L[halved_inds] < np.inf] # discard accidently failed models\n",
    "            T = T[halved_inds]\n",
    "            #print(T)\n",
    "    all_test_losses.append(test_losses)\n",
    "    all_wall_times.append(wall_times)\n",
    "alg_resutls['hyperband'] = {'all_test_losses':all_test_losses, 'all_wall_times':all_wall_times}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recepie_vectors = pd.read_csv('data/doc2vec_features.csv').set_index('recepie_id')\n",
    "df_recepie_vectors_lowdim = pd.read_csv('data/doc2vec_features_lowdim.csv').set_index('recepie_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_set_recepie_ids = np.array(env.get_recepie_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_highdim = df_recepie_vectors.loc[search_set_recepie_ids].values\n",
    "X_lowdim = df_recepie_vectors_lowdim.loc[search_set_recepie_ids].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, alias in zip([X_highdim, X_lowdim], ['50D', '10D']):\n",
    "#     if alias == '50D':\n",
    "#         continue\n",
    "    all_test_losses = []\n",
    "    all_wall_times = []\n",
    "    epochs_train = 50\n",
    "    N_init = 20 # check randomly a few architectures at first\n",
    "    beta = 2.0\n",
    "    N = int(1.3*iters_per_round)\n",
    "    train_batch = 10\n",
    "    for seed in tqdm_notebook(range(rounds)):\n",
    "    #for seed in tqdm_notebook(range(5)):\n",
    "        np.random.seed(seed)\n",
    "        env.reset()\n",
    "        selected_inds = []\n",
    "        test_losses = []\n",
    "        wall_times = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        # check a few random architectures at first\n",
    "        for i in range(N_init):\n",
    "            cur_ind = np.random.choice(np.setdiff1d(np.arange(len(search_set)), np.array(selected_inds)), \n",
    "                                       1, replace=False)[0]\n",
    "            env.simulated_train(search_set[cur_ind], epochs_train)\n",
    "            selected_inds.append(cur_ind)\n",
    "            if env.get_model_status(search_set[cur_ind]) == 'OK':\n",
    "                X_train.append(X[cur_ind])\n",
    "                y_train.append(env.get_model_stats(search_set[cur_ind], epochs_train - 1)['val_loss'])\n",
    "            test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "            wall_times.append(env.get_total_time())\n",
    "\n",
    "\n",
    "        regr = BaggingRegressor(XGBRegressor(n_estimators=100, max_depth=15), \n",
    "                                n_estimators=14, max_samples=0.5, n_jobs=14)\n",
    "\n",
    "        # train estimator and score new candidates according to the lower-confidence-bound acquisition function\n",
    "        for i in range(N_init, N):\n",
    "            if i % train_batch == 0:\n",
    "                regr.fit(np.array(X_train), np.array(y_train))\n",
    "                y_pred_mean = regr.predict(X)\n",
    "                y_pred_std = np.std([e.predict(X) for e in regr.estimators_], axis=0)\n",
    "                scores = y_pred_mean - beta * y_pred_std\n",
    "\n",
    "            scores[np.array(selected_inds)] = np.inf\n",
    "\n",
    "            cur_ind = np.argmin(scores)\n",
    "\n",
    "            env.simulated_train(search_set[cur_ind], epochs_train)\n",
    "            if env.get_model_status(search_set[cur_ind]) == 'OK':\n",
    "                X_train.append(X[cur_ind])\n",
    "                y_train.append(env.get_model_stats(search_set[cur_ind], epochs_train - 1)['val_loss'])\n",
    "            selected_inds.append(cur_ind)\n",
    "            test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "            wall_times.append(env.get_total_time())\n",
    "        all_test_losses.append(test_losses)\n",
    "        all_wall_times.append(wall_times)\n",
    "\n",
    "\n",
    "    alg_resutls[f'bayes_opt_{alias}'] = {'all_test_losses':all_test_losses, 'all_wall_times':all_wall_times}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_embedded(e, std=1, axes_bounds=None):\n",
    "    e_new = e + np.random.randn(len(e)) * std\n",
    "    if axes_bounds is not None:\n",
    "        e_new = np.clip(e_new, axes_bounds[0], axes_bounds[1])\n",
    "    return e_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(E, e):\n",
    "    #dists = np.linalg.norm(E - e.reshape(1, -1), axis=1)\n",
    "    dists = distance.cdist([e], E, \"cosine\")[0]\n",
    "    return np.argmin(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_losses = []\n",
    "all_wall_times = []\n",
    "\n",
    "train_epochs = 50\n",
    "P = 20\n",
    "C = int(1.3*iters_per_round)\n",
    "S = 10\n",
    "\n",
    "axes_bounds = (np.min(X, axis=0), np.max(X, axis=0))\n",
    "\n",
    "for seed in tqdm_notebook(range(rounds)):\n",
    "    np.random.seed(seed)\n",
    "    env.reset()\n",
    "    test_losses = []\n",
    "    wall_times = []\n",
    "    \n",
    "    # init first P architectures\n",
    "    population = []\n",
    "    history = []\n",
    "    for i in np.random.choice(np.arange(len(search_set)), P, replace=False):\n",
    "        env.simulated_train(search_set[i], train_epochs)\n",
    "        population.append(i)\n",
    "        history.append(i)\n",
    "        test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "        wall_times.append(env.get_total_time())\n",
    "    \n",
    "    attempt = 0\n",
    "    valid_round = True\n",
    "    while len(history) < C:\n",
    "        sample = np.random.choice(population, S, replace=False)\n",
    "        sample_scores = [env.get_model_stats(search_set[i], epochs_train - 1)['val_loss'] \n",
    "                         for i in sample if env.get_model_status(search_set[i]) == 'OK']\n",
    "        if len(sample_scores) == 0: \n",
    "            # this is unlikely to happen, but just to make sure that the code will work anyway\n",
    "            attempt += 1\n",
    "            if attempt > 5:\n",
    "                valid_round = False\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            attempt = 0\n",
    "        parent = sample[np.argmin(sample_scores)]\n",
    "        \n",
    "        for std in [0.5, 1.0, 2.0, 4.0, 8.0]:\n",
    "            e_new = mutate_embedded(X[parent], std, axes_bounds)\n",
    "            child = find_closest(X, e_new)\n",
    "            if child != parent:\n",
    "                # stop when we find a child that differs from the parent\n",
    "                break\n",
    "        \n",
    "        env.simulated_train(search_set[child], train_epochs)\n",
    "        history.append(child)\n",
    "        population = population[1:] + [child]\n",
    "        test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "        wall_times.append(env.get_total_time())\n",
    "    \n",
    "    if valid_round:\n",
    "        all_test_losses.append(test_losses)\n",
    "        all_wall_times.append(wall_times)\n",
    "    #break\n",
    "    \n",
    "alg_resutls['regularized_evolution'] = {'all_test_losses':all_test_losses, 'all_wall_times':all_wall_times}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_vec(vec):\n",
    "    \n",
    "    # The most similar vector in X:\n",
    "    distances = distance.cdist([vec], X, \"cosine\")[0]\n",
    "    #distances = np.linalg.norm(X - vec.reshape(1, -1), axis=1)\n",
    "    recepie_id = np.argmin(distances)\n",
    "    recepie = search_set[recepie_id]\n",
    "\n",
    "\n",
    "    env.simulated_train(recepie, epochs_train)\n",
    "    test_losses.append(env.get_test_loss_of_the_best_validated_architecture())\n",
    "    wall_times.append(env.get_total_time())\n",
    "\n",
    "\n",
    "    if env.get_model_status(recepie) == 'OK':\n",
    "        result = env.get_model_stats(recepie, epochs_train - 1)['val_loss']\n",
    "    else:\n",
    "        result = 10\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_dict(vec_as_dict):\n",
    "    \n",
    "    vec = np.zeros(50)\n",
    "    for k, v in vec_as_dict.items():\n",
    "        vec[int(k)] = v\n",
    "    \n",
    "    return objective_vec(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_highdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_space = {}\n",
    "for i in range(X.shape[-1]):\n",
    "    min_val = X[:, i].min()\n",
    "    max_val = X[:, i].max()\n",
    "    components_space[i] = hp.uniform(f'component_{i}', min_val, max_val)\n",
    "search_space = components_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(1.3*iters_per_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs_train in [50]:\n",
    "    all_test_losses = []\n",
    "    all_wall_times = []\n",
    "\n",
    "    for seed in tqdm_notebook(range(rounds)):\n",
    "        np.random.seed(seed)\n",
    "        os.environ['HYPEROPT_FMIN_SEED'] = str(seed)\n",
    "        env.reset()\n",
    "        test_losses, wall_times = [], []\n",
    "        \n",
    "        # minimize the objective over the space\n",
    "        best = fmin(objective_dict, search_space, algo=tpe.suggest, max_evals=N,\n",
    "                    verbose=False, show_progressbar=True, max_queue_len=20)\n",
    "        all_test_losses.append(test_losses)\n",
    "        all_wall_times.append(wall_times)\n",
    "\n",
    "    alg_resutls[f'TPE_{epochs_train}_epochs'] = {'all_test_losses':all_test_losses, 'all_wall_times':all_wall_times}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smac.facade.func_facade import fmin_smac\n",
    "from smac.initial_design.latin_hypercube_design import LHDesign\n",
    "import logging\n",
    "from ConfigSpace.hyperparameters import UniformFloatHyperparameter\n",
    "\n",
    "# Import ConfigSpace and different types of parameters\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from smac.facade.smac_hpo_facade import SMAC4HPO\n",
    "from smac.facade.smac_bo_facade import SMAC4BO\n",
    "from smac.initial_design.latin_hypercube_design import LHDesign\n",
    "from smac.optimizer.acquisition import LCB, EI, PI\n",
    "from smac.runhistory.runhistory2epm import RunHistory2EPM4InvScaledCost\n",
    "# Import SMAC-utilities\n",
    "from smac.scenario.scenario import Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smac.facade.smac_ac_facade import SMAC4AC\n",
    "from smac.scenario.scenario import Scenario\n",
    "from smac.tae.execute_ta_run import ExecuteTARun\n",
    "from smac.tae.execute_func import ExecuteTAFuncDict\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from smac.stats.stats import Stats\n",
    "from smac.initial_design.random_configuration_design import RandomConfigurations\n",
    "from smac.initial_design.latin_hypercube_design import LHDesign\n",
    "from ConfigSpace.hyperparameters import UniformFloatHyperparameter\n",
    "import json\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMACUtils(object):\n",
    "    def __init__(self, env, X=X, search_set=search_set,\n",
    "                 epochs_train=epochs_train):\n",
    "\n",
    "        self.env = env\n",
    "        self.X = X\n",
    "        self.search_set = search_set\n",
    "        self.epochs_train = epochs_train\n",
    "        self.stat = {}\n",
    "#         self.stat_file = stat_file\n",
    "        \n",
    "#         with open(self.stat_file, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "    \n",
    "    def objective_function(self, config):\n",
    "        vec = self._config_to_vec(config)\n",
    "        \n",
    "        distances = distance.cdist([vec], self.X, \"cosine\")[0]\n",
    "        recepie_id = np.argmin(distances)\n",
    "        recepie = self.search_set[recepie_id]\n",
    "        \n",
    "        self.env.simulated_train(recepie, self.epochs_train)\n",
    "        \n",
    "        test_loss = self.env.get_test_loss_of_the_best_validated_architecture()\n",
    "        wall_time = self.env.get_total_time()\n",
    "        self._collect_eval_stat(test_loss, wall_time)\n",
    "        \n",
    "        if self.env.get_model_status(recepie) == 'OK':\n",
    "            r = self.env.get_model_stats(recepie, self.epochs_train - 1)['val_loss']\n",
    "        else:\n",
    "            r = 1000\n",
    "        \n",
    "        return r, {\"test_loss\": test_loss, \"wall_time\": wall_time}\n",
    "\n",
    "        \n",
    "    def _config_to_vec(self, config):\n",
    "        vec_as_dict = config.get_dictionary()\n",
    "        vec = np.zeros(self.X.shape[-1])\n",
    "        for k, v in vec_as_dict.items():\n",
    "            vec[int(k)] = v\n",
    "        return vec\n",
    "    \n",
    "    def _collect_eval_stat(self, test_loss, wall_time):\n",
    "        stat = self.stat\n",
    "        \n",
    "        if 'test_losses' not in stat:\n",
    "            stat['test_losses'] = []\n",
    "        stat['test_losses'].append(test_loss)\n",
    "        \n",
    "        if 'wall_times' not in stat:\n",
    "            stat['wall_times'] = []\n",
    "        stat['wall_times'].append(wall_time)\n",
    "        \n",
    "        if 'eval_step' not in stat:\n",
    "            stat['eval_step'] = 0\n",
    "        stat['eval_step'] += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in [X_lowdim]:\n",
    "    axes_bounds = (np.min(X, axis=0), np.max(X, axis=0))\n",
    "    bounds = list(zip(axes_bounds[0], axes_bounds[1]))\n",
    "\n",
    "    cs = ConfigurationSpace()\n",
    "    cs.add_hyperparameters([\n",
    "        UniformFloatHyperparameter(str(i), X[:, i].min(), X[:, i].max(), default_value=0)\n",
    "        for i in range(X.shape[-1])\n",
    "    ]);\n",
    "\n",
    "    for initial_design in [LHDesign]: #[RandomConfigurations, LHDesign]:\n",
    "        all_test_losses = []\n",
    "        all_wall_times = []\n",
    "\n",
    "        for seed in range(rounds):\n",
    "            print(f\"START WITH INITIAL DESIGN: {initial_design.__name__} SEED: {seed}\")\n",
    "            scenario = Scenario({\"run_obj\": \"quality\",\n",
    "                             \"runcount-limit\": int(1.2*iters_per_round),\n",
    "                             \"wallclock-limit\": 3000,\n",
    "                             \"cs\": cs,\n",
    "                             \"deterministic\": \"true\",\n",
    "                             \"initial_incumbent\": \"RANDOM\",\n",
    "                             \"output_dir\": \"./tmp\",\n",
    "                             \"seed\": seed,\n",
    "                             \"limit_resources\": \"false\"})\n",
    "\n",
    "            env.reset()\n",
    "            b = SMACUtils(env, X=X)\n",
    "\n",
    "            def objective_function(config, **kwargs):\n",
    "                y, stat = b.objective_function(config)\n",
    "                return float(y)\n",
    "\n",
    "            stats = Stats(scenario=scenario)\n",
    "            smac = SMAC4AC(scenario=scenario,\n",
    "                           tae_runner=objective_function,\n",
    "                           initial_design=initial_design)\n",
    "            smac.optimize()\n",
    "\n",
    "            stat = b.stat\n",
    "            all_test_losses.append(stat['test_losses'])\n",
    "            all_wall_times.append(stat['wall_times'])\n",
    "\n",
    "        alg_resutls[f'SMAC[{initial_design.__name__}_{X.shape[-1]}D]'] = {'all_test_losses':all_test_losses, 'all_wall_times':all_wall_times}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot resuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_algs = {\n",
    "    'random_search_50_epochs': 'RS 50E',\n",
    "    'random_search_10_epochs': 'RS 10E',\n",
    "    'hyperband':'HB',\n",
    "    'bayes_opt_50D':'BO 50D',\n",
    "    'bayes_opt_10D':'BO 10D',\n",
    "    'regularized_evolution':'RE',\n",
    "    'TPE_50_epochs':'TPE',\n",
    "    'SMAC[LHDesign_10D]':'SMAC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_opt = env.get_best_possible_test_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "\n",
    "for i, alg_name in enumerate(['random_search_50_epochs', 'random_search_10_epochs', 'hyperband', \n",
    "                              'bayes_opt_50D', 'bayes_opt_10D', 'regularized_evolution', 'TPE_50_epochs',\n",
    "                             'SMAC[LHDesign_10D]']):\n",
    "    all_test_losses = alg_resutls[alg_name]['all_test_losses']\n",
    "    all_wall_times = alg_resutls[alg_name]['all_wall_times']\n",
    "\n",
    "    all_xs = np.array(all_test_losses)\n",
    "    all_ts = np.array(all_wall_times)/3600.\n",
    "\n",
    "    s = 1.96/np.sqrt(all_xs.shape[0])\n",
    "\n",
    "    all_ts_mean = all_ts.mean(axis=0)\n",
    "\n",
    "    all_ts_max = all_ts_mean + s*all_ts.std(axis=0)\n",
    "    all_ts_min = all_ts_mean - s*all_ts.std(axis=0)\n",
    "\n",
    "    all_xs_mean = np.nanmean(all_xs, axis=0)\n",
    "\n",
    "    all_xs_max = all_xs_mean + s*np.nanstd(all_xs, axis=0)\n",
    "    all_xs_min = all_xs_mean - s*np.nanstd(all_xs, axis=0)\n",
    "\n",
    "\n",
    "    plt.plot(all_ts_mean, all_xs_mean - y_opt, lw=1.5, color=f'C{i}', label=legend_algs[alg_name])\n",
    "\n",
    "    plt.fill_between(all_ts_mean, all_xs_min - y_opt, all_xs_max - y_opt, alpha=0.1, edgecolor=f'C{i}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Total train time [h]', fontsize=14)\n",
    "plt.ylabel('Regret', fontsize=14)\n",
    "plt.ylim([0.1, 1.0])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim([5, 1500])\n",
    "plt.xticks([10, 20, 50, 100, 200, 500, 1000]);\n",
    "plt.gca().get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "plt.savefig('data/figures/benchmarks_log_y_scale.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "\n",
    "plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "for i, alg_name in enumerate(['random_search_50_epochs', 'random_search_10_epochs', 'hyperband', \n",
    "                              'bayes_opt_50D', 'bayes_opt_10D', 'regularized_evolution', 'TPE_50_epochs',\n",
    "                             'SMAC[LHDesign_10D]']):\n",
    "    all_test_losses = alg_resutls[alg_name]['all_test_losses']\n",
    "    all_wall_times = alg_resutls[alg_name]['all_wall_times']\n",
    "\n",
    "    all_xs = np.array(all_test_losses)\n",
    "    all_ts = np.array(all_wall_times)/3600.\n",
    "\n",
    "    s = 1.96/np.sqrt(all_xs.shape[0])\n",
    "\n",
    "    all_ts_mean = all_ts.mean(axis=0)\n",
    "\n",
    "    all_ts_max = all_ts_mean + s*all_ts.std(axis=0)\n",
    "    all_ts_min = all_ts_mean - s*all_ts.std(axis=0)\n",
    "\n",
    "    all_xs_mean = np.nanmean(all_xs, axis=0)\n",
    "\n",
    "    all_xs_max = all_xs_mean + s*np.nanstd(all_xs, axis=0)\n",
    "    all_xs_min = all_xs_mean - s*np.nanstd(all_xs, axis=0)\n",
    "\n",
    "    plt.plot(np.sort(all_xs[:, -1]) - y_opt, np.linspace(0, 1, len(all_xs)), color=f'C{i}', label=legend_algs[alg_name])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.xlabel('Final test regret', fontsize=14)\n",
    "plt.ylabel('CDF', fontsize=14)\n",
    "plt.savefig('data/figures/benchmarks_CDF_regret.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
