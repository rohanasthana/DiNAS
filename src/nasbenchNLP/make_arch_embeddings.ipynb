{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import plotting\n",
    "import networkx as nx \n",
    "from joblib import Parallel, delayed\n",
    "import contextlib\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from utils import make_graph\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback:\n",
    "        def __init__(self, time, index, parallel):\n",
    "            self.index = index\n",
    "            self.parallel = parallel\n",
    "\n",
    "        def __call__(self, index):\n",
    "            tqdm_object.update()\n",
    "            if self.parallel._original_iterator is not None:\n",
    "                self.parallel.dispatch_next()\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = Path('train_logs_single_run')\n",
    "logs_paths = list(logs_path.glob(\"*.json\"))\n",
    "\n",
    "logs_ok = []\n",
    "logs_not_ok = []\n",
    "for idx, log_path in tqdm(enumerate(logs_paths), total=len(logs_paths)):\n",
    "    with open(log_path, \"r\") as f:\n",
    "        log = json.load(f)\n",
    "    recepie = json.loads(log['recepie'])\n",
    "    log['recepie'] = recepie\n",
    "    log['idx'] = idx\n",
    "    \n",
    "    if log['status'] == 'OK':\n",
    "        logs_ok.append(log)\n",
    "    else:\n",
    "        logs_not_ok.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number ok: \", len(logs_ok))\n",
    "print(\"number not ok: \", len(logs_not_ok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(logs_not_ok)/len(logs_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_node(x):\n",
    "    for v in ['node', 'h_prev', 'h_new']:\n",
    "        if x.find(v) != -1:\n",
    "            x = v\n",
    "    if x not in ['x', 'node', 'h_prev', 'h_new']: # to make lstm and gru recepies standard\n",
    "        x = 'node'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_2(recepie):\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    for n in recepie.keys():\n",
    "        if n not in G.nodes():\n",
    "            G.add_node(n)\n",
    "        for k in recepie[n]['input']:\n",
    "            if k not in G.nodes():\n",
    "                G.add_node(k)\n",
    "            G.add_edge(n, k, label=recepie[n]['op'])\n",
    "            G.add_edge(k, n, label='rev_' + recepie[n]['op'])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_features(G, steps=10):\n",
    "    walk = []\n",
    "    node = np.random.choice(G.nodes(), 1)[0]\n",
    "    for _ in range(steps):\n",
    "        k = np.random.choice(list(G.adj[node]), 1)[0]\n",
    "        walk.extend([map_node(node), G.adj[node][k]['label']])\n",
    "        node = k\n",
    "    walk.append(map_node(node))\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_sentences(G, sentences_num=20):\n",
    "    sentences = []\n",
    "    for _ in range(sentences_num):\n",
    "        sentences.extend(random_walk_features(G) + ['.'])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(log):\n",
    "    name = f\"log_{log['idx']}\"\n",
    "    recepie = log['recepie']\n",
    "    G = make_graph_2(recepie)\n",
    "    doc = TaggedDocument(words=make_graph_sentences(G), tags=[name])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_collections = Parallel(n_jobs=-2)(delayed(feature_extractor)(log) for log in tqdm(logs_not_ok+logs_ok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while!!\n",
    "size = 10\n",
    "# size = 50\n",
    "doc2vec_model = Doc2Vec(document_collections, \n",
    "                        size=size, window=3, dm=1, min_count=0, workers=8, epochs=100, hs=1,\n",
    "                        dbow_words=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for d in document_collections:\n",
    "    all_words |= set(d.words)\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dublicates_ok_logs = []\n",
    "not_dublicates_not_ok_logs = []\n",
    "not_dublicates_recepies = []\n",
    "for log in logs_not_ok+logs_ok:\n",
    "    recepie = log['recepie']\n",
    "    if recepie not in not_dublicates_recepies:\n",
    "        not_dublicates_recepies.append(recepie)\n",
    "        if log['status'] == 'OK':\n",
    "            not_dublicates_ok_logs.append(log)\n",
    "        else:\n",
    "            not_dublicates_not_ok_logs.append(log)\n",
    "print(\"total: \", len(logs_not_ok+logs_ok))\n",
    "print(\"without dublicates: \", len(not_dublicates_ok_logs+not_dublicates_not_ok_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump vectors\n",
    "recepie_id_vectors = {log['recepie_id']:doc2vec_model.docvecs[f\"log_{log['idx']}\"]\n",
    "                      for log in not_dublicates_ok_logs+not_dublicates_not_ok_logs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recepie_id_vectors_list = []\n",
    "for k in recepie_id_vectors:\n",
    "    k_dict = {'recepie_id':k}\n",
    "    for i in range(doc2vec_model.vector_size):\n",
    "        k_dict[f'v{i:02d}'] = recepie_id_vectors[k][i]\n",
    "    recepie_id_vectors_list.append(k_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recepie_vectors = pd.DataFrame(recepie_id_vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recepie_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if size == 50:\n",
    "#     df_recepie_vectors.to_csv('data/doc2vec_features.csv', index=False)\n",
    "# elif size == 10:\n",
    "#     df_recepie_vectors.to_csv('data/doc2vec_features_lowdim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recepie_vectors = pd.read_csv('data/doc2vec_features.csv').set_index('recepie_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_list = [\n",
    "    (df_recepie_vectors.loc[log['recepie_id']], int(log['status'] == 'OK'))\n",
    "    for log in not_dublicates_ok_logs+not_dublicates_not_ok_logs\n",
    "]\n",
    "\n",
    "random.shuffle(trainset_list)\n",
    "\n",
    "trainset_np, testset_np = np.array(trainset_list[:7000]), np.array(trainset_list[7000:])\n",
    "train_X, train_y = np.array(list(trainset_np[:, 0])), np.array(list(trainset_np[:, 1]))\n",
    "test_X, test_y = np.array(list(testset_np[:, 0])), np.array(list(testset_np[:, 1]))\n",
    "\n",
    "print(\"Train:\", len(trainset_np))\n",
    "print(\"Test: \", len(testset_np))\n",
    "\n",
    "num_train_not_ok = len(train_y) - train_y.sum()\n",
    "print(\"\\nTrain OK: \", train_y.sum())\n",
    "print(\"Train not OK: \", num_train_not_ok)\n",
    "\n",
    "num_test_not_ok = len(test_y) - test_y.sum()\n",
    "print(\"\\nTest OK: \", test_y.sum())\n",
    "print(\"Test not OK: \", num_test_not_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, n_iter=300, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = tsne.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(E[:, 0], E[:, 1], s=3, c=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, roc_curve, auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_jobs=-1)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "pred_y = model.predict_proba(test_X)\n",
    "precision_xgboost, recall_xgboost, _ = precision_recall_curve(test_y, pred_y[:, 1])\n",
    "fpr_xgboost, tpr_xgboost, _ = roc_curve(test_y, pred_y[:, 1])\n",
    "roc_auc_axboost = auc(fpr_xgboost, tpr_xgboost)\n",
    "\n",
    "f1_test_score = f1_score(test_y, np.argmax(pred_y, 1))\n",
    "ap_test_score = average_precision_score(test_y, pred_y[:, 1])\n",
    "print(\"XGBoost F1 score: \", f1_test_score)\n",
    "print(\"XGBoost Average Precision score: \", ap_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression().fit(train_X, train_y)\n",
    "pred_y = clf.predict_proba(test_X)\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(test_y, pred_y[:,1])\n",
    "fpr_lr, tpr_lr, _ = roc_curve(test_y, pred_y[:, 1])\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "f1_test_score = f1_score(test_y, np.argmax(pred_y, 1))\n",
    "ap_test_score = average_precision_score(test_y, pred_y[:, 1])\n",
    "print(\"Logistic Regression F1 score: \", f1_test_score)\n",
    "print(\"CatBoost Average Precision score: \", ap_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Receiver Operating Characteristic')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr_xgboost, tpr_xgboost, label='XGBoost AUC = %0.2f' % roc_auc_axboost)\n",
    "plt.plot(fpr_lr, tpr_lr, label='Logistic Regression AUC = %0.2f' % roc_auc_lr)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "\n",
    "plt.savefig('data/figures/prediction_faulty.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_list = [\n",
    "    (doc2vec_model.docvecs[f\"log_{log['idx']}\"], np.array(log['val_losses']).min())\n",
    "    for log in not_dublicates_ok_logs\n",
    "]\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.shuffle(trainset_list)\n",
    "\n",
    "trainset_np, testset_np = np.array(trainset_list[:7000]), np.array(trainset_list[7000:])\n",
    "train_X, train_y = np.array(list(trainset_np[:, 0])), np.array(list(trainset_np[:, 1]))\n",
    "test_X, test_y = np.array(list(testset_np[:, 0])), np.array(list(testset_np[:, 1]))\n",
    "\n",
    "print(\"Train:\", len(trainset_np))\n",
    "print(\"Test: \", len(testset_np))\n",
    "\n",
    "num_train_not_ok = len(train_y) - train_y.sum()\n",
    "print(\"\\nTrain OK: \", train_y.sum())\n",
    "print(\"Train not OK: \", num_train_not_ok)\n",
    "\n",
    "num_test_not_ok = len(test_y) - test_y.sum()\n",
    "print(\"\\nTest OK: \", test_y.sum())\n",
    "print(\"Test not OK: \", num_test_not_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(E[:, 0], E[:, 1], s=3, c=train_y, cmap=plt.cm.plasma_r)\n",
    "plt.colorbar()\n",
    "plt.clim([4.5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_y > 6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(E[:, 0], E[:, 1], s=3, color='C0')\n",
    "sub_inds = np.where(train_y > 6)[0]\n",
    "plt.scatter(E[sub_inds, 0], E[sub_inds, 1], s=5, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_jobs=-1)\n",
    "model.fit(train_X, train_y > 6)\n",
    "\n",
    "pred_y = model.predict_proba(test_X)\n",
    "precision_xgboost, recall_xgboost, _ = precision_recall_curve(test_y > 6, pred_y[:, 1])\n",
    "fpr_xgboost, tpr_xgboost, _ = roc_curve(test_y > 6, pred_y[:, 1])\n",
    "roc_auc_axboost = auc(fpr_xgboost, tpr_xgboost)\n",
    "\n",
    "f1_test_score = f1_score(test_y > 6, np.argmax(pred_y, 1))\n",
    "ap_test_score = average_precision_score(test_y > 6, pred_y[:, 1])\n",
    "print(\"XGBoost F1 score: \", f1_test_score)\n",
    "print(\"XGBoost Average Precision score: \", ap_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Receiver Operating Characteristic')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr_xgboost, tpr_xgboost, label='XGBoost AUC = %0.2f' % roc_auc_axboost)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "regr = BaggingRegressor(XGBRegressor(n_estimators=100, max_depth=15), n_jobs=10, n_estimators=20, max_samples=0.5).fit(train_X, train_y)\n",
    "regr_6 = BaggingRegressor(XGBRegressor(n_estimators=100, max_depth=15), n_jobs=10, n_estimators=20, max_samples=0.5).fit(train_X[train_y < 6], train_y[train_y < 6])\n",
    "pred_y = regr.predict(test_X)\n",
    "pred_y_6 = regr_6.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(test_y, pred_y, s=1)\n",
    "plt.xlabel('Testing log perplexity', fontsize=16)\n",
    "plt.ylabel('Predicted testing log perplexity', fontsize=16)\n",
    "plt.xlim([4.5, 7])\n",
    "plt.ylim([4.5, 7])\n",
    "plt.savefig('data/figures/prediction_loss.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(test_y, pred_y_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(test_y[test_y < 6], pred_y[test_y < 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(test_y[test_y < 6], pred_y_6[test_y < 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
