{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = []\n",
    "for fn in os.listdir('train_logs_single_run'):\n",
    "    if fn.endswith('.json'):\n",
    "        all_stats.append(json.load(open(os.path.join('train_logs_single_run', fn), 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_cnt(x):\n",
    "    all_nodes = set(x.keys())\n",
    "    for k in x.keys():\n",
    "        all_nodes |= set(x[k]['input'])\n",
    "    return len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([get_nodes_cnt(json.loads(x['recepie'])) for x in all_stats], bins=16, range=(4, 20))\n",
    "plt.xlabel('Number of nodes', fontsize=16)\n",
    "plt.ylabel('Number of architectures', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_stats = [x for x in all_stats if x['status'] == 'OK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recepie_ids = [x['recepie_id'] for x in ok_stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([np.exp(np.min(x['test_losses'])) for x in all_stats if x['status'] == 'OK'], \n",
    "         bins=50, range=(65, 250));\n",
    "plt.ylabel('Num. architectures', fontsize=14)\n",
    "plt.xlabel('Perplexity', fontsize=14)\n",
    "labels = ['RNN', 'LSTM', 'GRU']\n",
    "\n",
    "for i in range(3):\n",
    "    seek_id = recepie_ids.index(1000000 + i)\n",
    "    x = ok_stats[seek_id]\n",
    "    plt.vlines(np.exp(np.min(x['test_losses'])), 0, 1000, color=f'C{i+1}', label=labels[i], linestyle='--')\n",
    "plt.legend(fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.savefig('data/figures/ppl_distrib.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter([np.sum(x['wall_times']) for x in ok_stats], \n",
    "            [x['num_params'] for x in ok_stats], s=15,\n",
    "            c=[(np.min(np.exp(x['test_losses']))) for x in ok_stats],\n",
    "            cmap=plt.cm.viridis_r, alpha=0.99)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Test perplexity', fontsize=16)\n",
    "plt.clim([65, 400])\n",
    "\n",
    "labels = ['RNN', 'LSTM', 'GRU']\n",
    "markers = ['X', '^', 'o']\n",
    "for i in range(3):\n",
    "    seek_id = recepie_ids.index(1000000 + i)\n",
    "    x = ok_stats[seek_id]\n",
    "    plt.scatter([np.sum(x['wall_times'])],\n",
    "                [x['num_params']],\n",
    "                c='r', marker=markers[i], zorder=10, edgecolor='k', lw=0.5,\n",
    "                s=200, label=labels[i])\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel('Wall time [s]', fontsize=16)\n",
    "plt.ylabel('Num params', fontsize=16)\n",
    "plt.savefig('data/figures/main_metrics.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(x):\n",
    "    r = np.zeros_like(x)\n",
    "    r[np.argsort(x)] = np.arange(len(x))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([x['test_losses'] for x in all_stats if x['status'] == 'OK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x['val_losses'] for x in all_stats if x['status'] == 'OK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 5))\n",
    "for i, e in enumerate([5, 10, 25, 50]):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.scatter(get_rank(X[:, e - 1]), get_rank(Y[:, -1]), s=1)\n",
    "    plt.xlabel(f'Validation rank {e} epoch', fontsize=14)\n",
    "    plt.ylabel('Test rank 50 epoch', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/figures/dynamic_ranking.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with performance on wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in os.listdir('train_logs_multi_runs'):\n",
    "    if fn.endswith('.json'):\n",
    "        all_stats.append(json.load(open(os.path.join('train_logs_multi_runs', fn), 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_wiki = []\n",
    "for fn in os.listdir('train_logs_wikitext-2'):\n",
    "    if fn.endswith('.json'):\n",
    "        all_stats_wiki.append(json.load(open(os.path.join('train_logs_wikitext-2', fn), 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_stats_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_stats_wiki = [x for x in all_stats_wiki if x['status'] == 'OK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_ppl = {x['recepie_id']:np.exp(np.min(x['test_losses'])) for x in all_stats if x['status'] == 'OK'}\n",
    "id_to_ppl_wiki = {x['recepie_id']:np.exp(np.min(x['test_losses'])) for x in all_stats_wiki if x['status'] == 'OK'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = []\n",
    "ppl_wiki = []\n",
    "for k in id_to_ppl_wiki:\n",
    "    if k in id_to_ppl:\n",
    "        ppl.append(id_to_ppl[k])\n",
    "        ppl_wiki.append(id_to_ppl_wiki[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(np.log(ppl), np.log(ppl_wiki))\n",
    "plt.xlabel('PTB testing log perplexity', fontsize=16)\n",
    "plt.ylabel('WikiText-2 testing log perplexity', fontsize=16)\n",
    "plt.savefig('data/figures/transfer_corr.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
